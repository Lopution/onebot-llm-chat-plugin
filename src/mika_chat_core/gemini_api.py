"""Gemini API å®¢æˆ·ç«¯æ¨¡å—ï¼ˆOpenAI å…¼å®¹æ ¼å¼ï¼‰ã€‚

é€šè¿‡ OpenAI å…¼å®¹æ ¼å¼ API è°ƒç”¨ Gemini æ¨¡å‹çš„å¼‚æ­¥å®¢æˆ·ç«¯ï¼Œæ”¯æŒï¼š
- å¤šè½®å¯¹è¯ï¼ˆä¸Šä¸‹æ–‡ç®¡ç†ï¼‰
- æµå¼å“åº”ä¸æ™®é€šå“åº”
- å¤š API Key æ™ºèƒ½è½®è¯¢ï¼ˆé™æµè‡ªåŠ¨æ¢å¤ï¼‰
- Tool Callingï¼ˆå‡½æ•°è°ƒç”¨ï¼‰
- å›¾ç‰‡ç†è§£ï¼ˆå¤šæ¨¡æ€è¾“å…¥ï¼‰

æ³¨æ„ï¼š
- æœ¬æ¨¡å—ä½¿ç”¨ OpenAI å…¼å®¹æ ¼å¼çš„ API ç«¯ç‚¹ï¼Œè€Œé Gemini åŸç”Ÿ SDK
- ä¾èµ– httpx è¿›è¡Œå¼‚æ­¥ HTTP è¯·æ±‚
- ä¸Šä¸‹æ–‡å­˜å‚¨å¯é€‰æ‹©å†…å­˜æˆ– SQLite åç«¯
- æ”¯æŒç”¨æˆ·æ¡£æ¡ˆå’Œå›¾ç‰‡å¤„ç†ç­‰å¯é€‰åŠŸèƒ½
"""
from __future__ import annotations

import asyncio
import httpx
import uuid
import time
import random
from datetime import datetime
from typing import Optional, List, Union, Dict, Any, Callable, Tuple

from nonebot import logger as log

# å¯¼å…¥å¼‚å¸¸ç±»
from .errors import GeminiAPIError, RateLimitError, AuthenticationError, ServerError

# å¯¼å…¥æŒä¹…åŒ–ä¸Šä¸‹æ–‡å­˜å‚¨ï¼ˆå¯é€‰ï¼‰
try:
    from .utils.context_store import get_context_store, SQLiteContextStore
    HAS_SQLITE_STORE = True
except ImportError:
    HAS_SQLITE_STORE = False

from .utils.prompt_loader import load_judge_prompt
from .config import plugin_config
from .gemini_api_sanitize import clean_thinking_markers
from .gemini_api_proactive import extract_json_object
from .gemini_api_messages import PreSearchResult, pre_search, build_messages
from .gemini_api_tools import ToolLoopResult, handle_tool_calls
from .gemini_api_transport import send_api_request
from .metrics import metrics

# å¯¼å…¥ç”¨æˆ·æ¡£æ¡ˆå­˜å‚¨ï¼ˆå¯é€‰ï¼‰
try:
    from .utils.user_profile import get_user_profile_store, UserProfileStore
    HAS_USER_PROFILE = True
except ImportError:
    HAS_USER_PROFILE = False
    # å¯é€‰ä¾èµ–ç¼ºå¤±æ—¶ï¼Œé¿å…åœ¨ chat ç›¸å…³é€»è¾‘ä¸­å¼•ç”¨æœªå®šä¹‰åç§°
    get_user_profile_store = None  # type: ignore[assignment]
    UserProfileStore = None  # type: ignore[assignment]

# å¯¼å…¥å›¾ç‰‡å¤„ç†å™¨ï¼ˆå¯é€‰ï¼‰
try:
    from .utils.image_processor import get_image_processor, ImageProcessor, ImageProcessError
    HAS_IMAGE_PROCESSOR = True
except ImportError:
    HAS_IMAGE_PROCESSOR = False
    # å¯é€‰ä¾èµ–ç¼ºå¤±æ—¶ï¼Œé¿å…åœ¨ chat ç›¸å…³é€»è¾‘ä¸­å¼•ç”¨æœªå®šä¹‰åç§°
    get_image_processor = None  # type: ignore[assignment]
    ImageProcessor = None  # type: ignore[assignment]
    ImageProcessError = None  # type: ignore[assignment]


# ==================== Magic-number constants ====================
# è¯´æ˜ï¼šä»…å°†æ•£è½çš„ç¡¬ç¼–ç æ•°å€¼æå–ä¸ºâ€œå‘½åå¸¸é‡/é…ç½®é¡¹â€ï¼Œä¸æ”¹å˜é»˜è®¤è¡Œä¸ºã€‚
UUID_SHORT_ID_LENGTH = 8

# å†…å­˜ä¸Šä¸‹æ–‡ï¼šå…è®¸çš„æœ€å¤§å†å²æ¡æ•° = max_context * 2ï¼ˆä¸åŸé€»è¾‘ä¸€è‡´ï¼‰
CONTEXT_HISTORY_MULTIPLIER = 2

# è¯Šæ–­/æ—¥å¿—é¢„è§ˆé•¿åº¦
CONTEXT_DIAGNOSTIC_TAIL_COUNT = 3
HISTORY_MESSAGE_PREVIEW_CHARS = 80
API_CONTENT_DEBUG_MIN_CHARS = 200
API_CONTENT_DEBUG_PREVIEW_CHARS = 500
RAW_MODEL_REPLY_PREVIEW_CHARS = 300
ERROR_RESPONSE_BODY_PREVIEW_CHARS = 500

# chat é»˜è®¤é‡è¯•ä¸é™çº§
DEFAULT_CHAT_RETRY_COUNT = 2
MAX_CONTEXT_DEGRADATION_LEVEL = 2
EMPTY_REPLY_RETRY_DELAY_SECONDS = 0.5

# æŒ‡æ•°é€€é¿ï¼šwait_time = 2 ** (2 - retry_count)
SERVER_ERROR_RETRY_BACKOFF_BASE = 2
SERVER_ERROR_RETRY_EXPONENT_OFFSET = 2

# ä¸»åŠ¨å‘è¨€åˆ¤å†³ï¼šæ—¥å¿—æˆªæ–­
PROACTIVE_JUDGE_ERROR_PREVIEW_CHARS = 100
PROACTIVE_JUDGE_RAW_CONTENT_SHORT_PREVIEW_CHARS = 100
PROACTIVE_JUDGE_RAW_CONTENT_ERROR_PREVIEW_CHARS = 200
PROACTIVE_JUDGE_SERVER_RESPONSE_PREVIEW_CHARS = 500


class GeminiClient:
    """å¼‚æ­¥ Gemini API å®¢æˆ·ç«¯ï¼ˆå…¼å®¹ OpenAI æ ¼å¼ï¼‰ã€‚

    åŠŸèƒ½ç‰¹æ€§ï¼š
    - å¤š API Key æ™ºèƒ½è½®è¯¢ï¼ˆé™æµè‡ªåŠ¨æ¢å¤ï¼‰
    - ä¸Šä¸‹æ–‡æŒä¹…åŒ–å­˜å‚¨ï¼ˆå†…å­˜/SQLiteï¼‰
    - Tool Calling æ”¯æŒï¼ˆweb_searchã€ç¾¤å†å²æœç´¢ç­‰ï¼‰
    - å¤šæ¨¡æ€è¾“å…¥ï¼ˆæ–‡æœ¬+å›¾ç‰‡ï¼‰
    - è‡ªåŠ¨é‡è¯•å’Œä¸Šä¸‹æ–‡é™çº§

    Attributes:
        api_key: ä¸» API Key
        model: é»˜è®¤æ¨¡å‹åç§°
        system_prompt: ç³»ç»Ÿæç¤ºè¯
        name: è§’è‰²åç§°ï¼ˆç”¨äºé”™è¯¯æ¶ˆæ¯æ¨¡æ¿ï¼‰
    """
    
    # é»˜è®¤é”™è¯¯æ¶ˆæ¯æ¨¡æ¿ - ä½¿ç”¨ {name} å ä½ç¬¦æ”¯æŒå¤šè§’è‰²
    # å¯é€šè¿‡æ„é€ å‡½æ•°ä¼ å…¥ error_messages å‚æ•°è¦†ç›–
    DEFAULT_ERROR_MESSAGES = {
        "timeout": "å‘œâ€¦{name} çš„è„‘è¢‹è½¬å¾—å¤ªæ…¢äº†ï¼Œç­‰ä¼šå„¿å†è¯•è¯•å‘¢~",
        "rate_limit": "å“‡â€¦å¤§å®¶éƒ½åœ¨æ‰¾ {name} èŠå¤©ï¼Œæœ‰ç‚¹å¿™ä¸è¿‡æ¥äº†ï¼Œç¨ç­‰ä¸€ä¸‹ä¸‹å¥½å—ï¼ŸğŸ’¦",
        "auth_error": "è¯¶ï¼Ÿ{name} çš„èº«ä»½éªŒè¯å‡ºäº†ç‚¹é—®é¢˜ï¼Œå¿«è®© Sensei å¸®å¿™æ£€æŸ¥ä¸€ä¸‹~",
        "server_error": "æœåŠ¡å™¨é‚£è¾¹å¥½åƒåœ¨æ‰“çŒç¡â€¦å†è¯•ä¸€æ¬¡å§~",
        "content_filter": "å””â€¦è¿™ä¸ªè¯é¢˜ {name} ä¸å¤ªæ–¹ä¾¿å›ç­”å‘¢â€¦æ¢ä¸ªè¯é¢˜å§ï¼Ÿ",
        "api_error": "è¯¶ï¼Ÿå¥½åƒå‡ºäº†ç‚¹å°é—®é¢˜â€¦{name} éœ€è¦ä¼‘æ¯ä¸€ä¸‹~",
        "unknown": "å•Šå’§ï¼Ÿå‘ç”Ÿäº†å¥‡æ€ªçš„äº‹æƒ…â€¦å†è¯´ä¸€æ¬¡å¥½ä¸å¥½ï¼Ÿ",
        "empty_reply": "å‘ï¼Œ{name} åˆšæ‰èµ°ç¥äº†ï¼Œèƒ½å†è¯´ä¸€æ¬¡å—ï¼Ÿâ˜†"
    }
    
    # å¯ç”¨å·¥å…·å®šä¹‰ï¼ˆOpenAI æ ¼å¼ï¼‰
    AVAILABLE_TOOLS = [
        {
            "type": "function",
            "function": {
                "name": "web_search",
                "description": "æœç´¢äº’è”ç½‘è·å–å®æ—¶ä¿¡æ¯ã€‚å½“ç”¨æˆ·è¯¢é—®æœ€æ–°æ–°é—»ã€å¤©æ°”ã€æ¯”èµ›ç»“æœã€ä»·æ ¼ã€è‚¡ç¥¨ã€å®æ—¶äº‹ä»¶ç­‰æ—¶æ•ˆæ€§ä¿¡æ¯æ—¶ä½¿ç”¨ã€‚æ³¨æ„ï¼šä¸è¦ç”¨äºæŸ¥è¯¢ç¾¤èŠå†å²æˆ–å¯¹è¯ä¸Šä¸‹æ–‡ã€‚",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "query": {
                            "type": "string",
                            "description": "æœç´¢å…³é”®è¯ï¼Œåº”ç®€æ´æ˜ç¡®"
                        }
                    },
                    "required": ["query"]
                }
            }
        },
        {
            "type": "function",
            "function": {
                "name": "search_group_history",
                "description": "æœç´¢å½“å‰ç¾¤èŠçš„å†å²æ¶ˆæ¯è®°å½•ã€‚å½“ç”¨æˆ·è¯¢é—®'ç¾¤é‡Œåˆšæ‰åœ¨èŠä»€ä¹ˆ'ã€'ä¹‹å‰è¯´äº†ä»€ä¹ˆ'ç­‰å…³äºç¾¤èŠä¸Šä¸‹æ–‡çš„é—®é¢˜æ—¶ä½¿ç”¨ã€‚ä»…åœ¨ç¾¤èŠä¸­å¯ç”¨ã€‚",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "count": {
                            "type": "integer",
                            "description": "è¦è·å–çš„å†å²æ¶ˆæ¯æ•°é‡ï¼Œé»˜è®¤20ï¼Œæœ€å¤§50"
                        }
                    }
                }
            }
        },
        {
            "type": "function",
            "function": {
                "name": "fetch_history_images",
                "description": "è·å–å†å²æ¶ˆæ¯ä¸­çš„å›¾ç‰‡ã€‚å½“ä½ éœ€è¦æŸ¥çœ‹ä¹‹å‰å¯¹è¯ä¸­æåˆ°çš„å›¾ç‰‡ï¼ˆå¦‚è¡¨æƒ…åŒ…ã€æˆªå›¾ï¼‰æ—¶ä½¿ç”¨ã€‚ä¸Šä¸‹æ–‡ä¸­å¸¦æœ‰ <msg_id:xxx> æ ‡è®°çš„æ¶ˆæ¯å¯ä»¥é€šè¿‡æ­¤å·¥å…·è·å–åŸå›¾ã€‚",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "msg_ids": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "éœ€è¦è·å–å›¾ç‰‡çš„æ¶ˆæ¯ ID åˆ—è¡¨ï¼ˆä»ä¸Šä¸‹æ–‡ä¸­çš„ <msg_id:xxx> æå–ï¼‰"
                        },
                        "max_images": {
                            "type": "integer",
                            "description": "æœ€å¤šè·å–å‡ å¼ å›¾ç‰‡ï¼Œé»˜è®¤2ï¼Œæœ€å¤§2"
                        }
                    },
                    "required": ["msg_ids"]
                }
            }
        }
    ]
    
    def __init__(
        self,
        api_key: str,
        base_url: str = "https://generativelanguage.googleapis.com/v1beta/openai",
        model: str = "gemini-3-flash",
        system_prompt: str = "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹",
        max_context: int = 10,
        api_key_list: Optional[List[str]] = None,
        use_persistent_storage: bool = True,  # æ˜¯å¦ä½¿ç”¨æŒä¹…åŒ–å­˜å‚¨
        character_name: str = "Mika",  # è§’è‰²åç§°ï¼Œç”¨äºé”™è¯¯æ¶ˆæ¯
        error_messages: Optional[Dict[str, str]] = None,  # è‡ªå®šä¹‰é”™è¯¯æ¶ˆæ¯æ¨¡æ¿
        enable_smart_search: bool = False  # æ˜¯å¦å¯ç”¨æ™ºèƒ½æœç´¢ï¼ˆLLM æ„å›¾è¯†åˆ«ï¼‰
    ):
        """åˆå§‹åŒ– Gemini API å®¢æˆ·ç«¯ã€‚

        åˆ›å»ºä¸€ä¸ªä¸ Gemini APIï¼ˆOpenAI å…¼å®¹æ ¼å¼ï¼‰äº¤äº’çš„å®¢æˆ·ç«¯å®ä¾‹ï¼Œ
        æ”¯æŒå¤š API Key è½®è¯¢ã€æŒä¹…åŒ–ä¸Šä¸‹æ–‡å­˜å‚¨ã€å·¥å…·è°ƒç”¨ç­‰é«˜çº§åŠŸèƒ½ã€‚

        Args:
            api_key: ä¸» API Keyï¼Œå½“ api_key_list ä¸ºç©ºæ—¶ä½¿ç”¨ã€‚
            base_url: API åŸºç¡€ URLï¼Œé»˜è®¤ä¸º Google Gemini çš„ OpenAI å…¼å®¹ç«¯ç‚¹ã€‚
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸º "gemini-3-flash"ã€‚
            system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼Œå®šä¹‰ AI è§’è‰²å’Œè¡Œä¸ºã€‚
            max_context: æœ€å¤§ä¸Šä¸‹æ–‡æ¶ˆæ¯æ•°é‡ï¼Œè¶…å‡ºæ—¶è‡ªåŠ¨æˆªæ–­å†å²ã€‚
            api_key_list: å¤š API Key åˆ—è¡¨ï¼Œå¯ç”¨æ™ºèƒ½è½®è¯¢å’Œå†·å´æœºåˆ¶ã€‚
            use_persistent_storage: æ˜¯å¦ä½¿ç”¨ SQLite æŒä¹…åŒ–å­˜å‚¨ä¸Šä¸‹æ–‡ï¼Œ
                è®¾ä¸º False åˆ™ä½¿ç”¨å†…å­˜å­˜å‚¨ã€‚
            character_name: è§’è‰²åç§°ï¼Œç”¨äºæ ¼å¼åŒ–é”™è¯¯æ¶ˆæ¯ä¸­çš„ {name} å ä½ç¬¦ã€‚
            error_messages: è‡ªå®šä¹‰é”™è¯¯æ¶ˆæ¯æ¨¡æ¿å­—å…¸ï¼Œå¯è¦†ç›– DEFAULT_ERROR_MESSAGESã€‚
            enable_smart_search: æ˜¯å¦å¯ç”¨æ™ºèƒ½æœç´¢ï¼ˆåŸºäº LLM æ„å›¾è¯†åˆ«ï¼‰ï¼Œ
                ä¼šå¢åŠ  API è°ƒç”¨æ¬¡æ•°ä½†æé«˜æœç´¢å‡†ç¡®æ€§ã€‚

        Example:
            >>> client = GeminiClient(
            ...     api_key="your-api-key",
            ...     model="gemini-2.0-flash",
            ...     character_name="Mika",
            ...     enable_smart_search=True
            ... )
        """
        self.api_key = api_key
        self.api_key_list = api_key_list or []
        self.base_url = base_url.rstrip("/")
        self.model = model
        self.system_prompt = system_prompt
        self.max_context = max_context
        self._enable_smart_search = enable_smart_search
        
        # è§’è‰²åç§°å’Œé”™è¯¯æ¶ˆæ¯é…ç½®
        self._character_name = character_name
        self._error_messages = error_messages or self.DEFAULT_ERROR_MESSAGES.copy()
        
        # é€‰æ‹©å­˜å‚¨åç«¯
        self._use_persistent = use_persistent_storage and HAS_SQLITE_STORE
        if self._use_persistent:
            cache_size = max(
                1,
                int(getattr(plugin_config, "gemini_context_cache_max_size", 200) or 200),
            )
            self._context_store: Optional[SQLiteContextStore] = get_context_store(
                max_context,
                max_cache_size=cache_size,
                context_mode=str(getattr(plugin_config, "gemini_context_mode", "structured")),
                max_turns=int(getattr(plugin_config, "gemini_context_max_turns", 30) or 30),
                max_tokens_soft=int(
                    getattr(plugin_config, "gemini_context_max_tokens_soft", 12000) or 12000
                ),
                summary_enabled=bool(
                    getattr(plugin_config, "gemini_context_summary_enabled", False)
                ),
                history_store_multimodal=bool(
                    getattr(plugin_config, "gemini_history_store_multimodal", False)
                ),
            )
            log.info("ä½¿ç”¨ SQLite æŒä¹…åŒ–ä¸Šä¸‹æ–‡å­˜å‚¨")
        else:
            self._context_store = None
            # å†…å­˜å­˜å‚¨ä½œä¸ºåå¤‡
            from collections import defaultdict
            self._contexts: Dict[tuple, List[Dict[str, Any]]] = defaultdict(list)
            log.info("ä½¿ç”¨å†…å­˜ä¸Šä¸‹æ–‡å­˜å‚¨")
        
        # å¤ç”¨ httpx å®¢æˆ·ç«¯
        self._http_client: Optional[httpx.AsyncClient] = None
        self._client_lock: asyncio.Lock = asyncio.Lock()
        
        # å·¥å…·æ‰§è¡Œå™¨ä¸å®šä¹‰
        self._tool_handlers: Dict[str, Callable] = {}
        
        self._key_index = 0  # è½®è¯¢ç´¢å¼•
        # æ™ºèƒ½è½®è¯¢ï¼šè®°å½•è¢«é™æµçš„ Key åŠå…¶å†·å´ç»“æŸæ—¶é—´
        self._key_cooldowns: Dict[str, float] = {}
        self._default_cooldown = plugin_config.gemini_api_key_default_cooldown_seconds  # é»˜è®¤å†·å´æ—¶é—´ï¼ˆç§’ï¼‰
        
    @property
    def is_persistent(self) -> bool:
        """æ˜¯å¦å¯ç”¨äº†æŒä¹…åŒ–å­˜å‚¨"""
        return self._use_persistent
        
    def _get_api_key(self) -> str:
        """è·å–å½“å‰è¦ä½¿ç”¨çš„ API Keyï¼ˆæ™ºèƒ½è½®è¯¢ï¼Œè·³è¿‡å†·å´ä¸­çš„ Keyï¼‰ã€‚

        å®ç°æ™ºèƒ½ API Key è½®è¯¢ç­–ç•¥ï¼š
        1. å¦‚æœæœªé…ç½®å¤š Keyï¼ˆapi_key_list ä¸ºç©ºï¼‰ï¼Œè¿”å›ä¸» api_key
        2. éå† api_key_listï¼Œè·³è¿‡å¤„äºå†·å´æœŸçš„ Key
        3. å¦‚æœæ‰€æœ‰ Key éƒ½åœ¨å†·å´æœŸï¼Œè¿”å›å†·å´æ—¶é—´æœ€çŸ­çš„é‚£ä¸ª

        æ­¤æœºåˆ¶é…åˆ _mark_key_rate_limited å®ç°é™æµè‡ªåŠ¨æ¢å¤ã€‚

        Returns:
            å½“å‰å¯ç”¨çš„ API Key å­—ç¬¦ä¸²ã€‚

        Note:
            å†·å´æœŸç”± _mark_key_rate_limited è®¾ç½®ï¼Œé»˜è®¤ä¸º 60 ç§’ï¼Œ
            æˆ– API è¿”å›çš„ Retry-After å¤´æŒ‡å®šçš„æ—¶é—´ã€‚
        """
        current_time = time.time()
        
        # å¦‚æœæ²¡æœ‰é…ç½®å¤šä¸ª Keyï¼Œä½¿ç”¨ä¸» Key
        if not self.api_key_list:
            return self.api_key
        
        # å°è¯•æ‰¾åˆ°ä¸€ä¸ªä¸åœ¨å†·å´æœŸçš„ Key
        all_keys = self.api_key_list.copy()
        attempts = 0
        max_attempts = len(all_keys)
        
        while attempts < max_attempts:
            key = all_keys[self._key_index]
            self._key_index = (self._key_index + 1) % len(all_keys)
            attempts += 1
            
            # æ£€æŸ¥æ˜¯å¦åœ¨å†·å´æœŸ
            cooldown_end = self._key_cooldowns.get(key, 0)
            if current_time >= cooldown_end:
                # ä¸åœ¨å†·å´æœŸï¼Œå¯ä»¥ä½¿ç”¨
                if key in self._key_cooldowns:
                    del self._key_cooldowns[key]  # æ¸…é™¤è¿‡æœŸçš„å†·å´è®°å½•
                    log.debug(f"API Key #{self._key_index} å†·å´æœŸç»“æŸï¼Œæ¢å¤ä½¿ç”¨")
                return key
            else:
                remaining = int(cooldown_end - current_time)
                log.debug(f"API Key #{self._key_index} ä»åœ¨å†·å´ä¸­ï¼ˆå‰©ä½™ {remaining}sï¼‰ï¼Œè·³è¿‡")
        
        # æ‰€æœ‰ Key éƒ½åœ¨å†·å´æœŸï¼Œé€‰æ‹©å†·å´æ—¶é—´æœ€çŸ­çš„
        min_cooldown_key = min(all_keys, key=lambda k: self._key_cooldowns.get(k, 0))
        log.warning(f"æ‰€æœ‰ API Key éƒ½åœ¨å†·å´æœŸï¼Œå¼ºåˆ¶ä½¿ç”¨å†·å´æ—¶é—´æœ€çŸ­çš„ Key")
        return min_cooldown_key
    
    def _mark_key_rate_limited(self, key: str, retry_after: int = 0):
        """æ ‡è®° API Key è¢«é™æµï¼Œè¿›å…¥å†·å´æœŸã€‚

        å½“æ”¶åˆ° 429 é™æµå“åº”æ—¶è°ƒç”¨æ­¤æ–¹æ³•ï¼Œå°†è¯¥ Key åŠ å…¥å†·å´é˜Ÿåˆ—ã€‚
        å†·å´æœŸç»“æŸå‰ï¼Œ_get_api_key ä¼šè·³è¿‡æ­¤ Keyã€‚

        Args:
            key: è¢«é™æµçš„ API Keyã€‚
            retry_after: API è¿”å›çš„ Retry-After ç§’æ•°ï¼Œ0 è¡¨ç¤ºä½¿ç”¨é»˜è®¤å†·å´æ—¶é—´ã€‚

        Note:
            é»˜è®¤å†·å´æ—¶é—´ä¸º 60 ç§’ï¼ˆself._default_cooldownï¼‰ã€‚
            å†·å´ä¿¡æ¯å­˜å‚¨åœ¨ self._key_cooldowns å­—å…¸ä¸­ã€‚
        """
        cooldown_seconds = retry_after if retry_after > 0 else self._default_cooldown
        self._key_cooldowns[key] = time.time() + cooldown_seconds
        log.warning(f"API Key è¢«é™æµï¼Œè¿›å…¥å†·å´æœŸ {cooldown_seconds}s")
    
    def register_tool_handler(self, name: str, handler: Callable):
        """æ³¨å†Œå·¥å…·å¤„ç†å™¨"""
        self._tool_handlers[name] = handler
    
    def _get_error_message(self, error_type: str) -> str:
        """
        è·å–æ ¼å¼åŒ–åçš„é”™è¯¯æ¶ˆæ¯
        
        Args:
            error_type: é”™è¯¯ç±»å‹ï¼ˆtimeout, rate_limit, auth_error ç­‰ï¼‰
            
        Returns:
            æ ¼å¼åŒ–åçš„é”™è¯¯æ¶ˆæ¯å­—ç¬¦ä¸²
        """
        template = self._error_messages.get(error_type, self._error_messages.get("unknown", "å‘ç”Ÿäº†é”™è¯¯"))
        try:
            return template.format(name=self._character_name)
        except KeyError:
            # å¦‚æœæ¨¡æ¿ä¸­æœ‰å…¶ä»–æœªçŸ¥å ä½ç¬¦ï¼Œè¿”å›åŸå§‹æ¨¡æ¿
            return template
    
    @property
    def character_name(self) -> str:
        """è·å–è§’è‰²åç§°"""
        return self._character_name
    
    @character_name.setter
    def character_name(self, value: str):
        """è®¾ç½®è§’è‰²åç§°"""
        self._character_name = value
    
    @property
    def context_store(self) -> Optional[SQLiteContextStore]:
        """è·å–ä¸Šä¸‹æ–‡å­˜å‚¨å®ä¾‹ï¼ˆåªè¯»ï¼‰"""
        return self._context_store
    
    async def _get_client(self) -> httpx.AsyncClient:
        """è·å–æˆ–åˆ›å»º httpx å®¢æˆ·ç«¯"""
        async with self._client_lock:
            if self._http_client is None or self._http_client.is_closed:
                # è¶…æ—¶æ—¶é—´ï¼ˆé»˜è®¤ 120sï¼‰ï¼Œé˜²æ­¢æ¨¡å‹æ€è€ƒè¿‡ä¹…å¯¼è‡´è¿æ¥æ–­å¼€
                self._http_client = httpx.AsyncClient(timeout=plugin_config.gemini_http_client_timeout_seconds)
            return self._http_client
    
    async def close(self):
        """å…³é—­ httpx å®¢æˆ·ç«¯"""
        async with self._client_lock:
            if self._http_client and not self._http_client.is_closed:
                await self._http_client.aclose()
                self._http_client = None
    
    def _get_context_key(self, user_id: str, group_id: Optional[str] = None) -> tuple:
        if group_id:
            return (group_id, "GROUP_CHAT")
        return ("PRIVATE_CHAT", user_id)
    
    async def _get_context_async(self, user_id: str, group_id: Optional[str] = None) -> List[Dict[str, Any]]:
        """å¼‚æ­¥è·å–ä¸Šä¸‹æ–‡ï¼ˆæ”¯æŒæŒä¹…åŒ–å­˜å‚¨ï¼‰"""
        if self._use_persistent and self._context_store:
            return await self._context_store.get_context(user_id, group_id)
        else:
            key = self._get_context_key(user_id, group_id)
            return self._contexts[key]
    
    
    async def _add_to_context_async(
        self,
        user_id: str,
        role: str,
        content: Union[str, List[Dict[str, Any]]],
        group_id: Optional[str] = None,
        message_id: Optional[str] = None,
        tool_calls: Optional[List[Dict[str, Any]]] = None,
        tool_call_id: Optional[str] = None,
    ):
        """å¼‚æ­¥æ·»åŠ æ¶ˆæ¯åˆ°ä¸Šä¸‹æ–‡ï¼ˆæ”¯æŒæŒä¹…åŒ–å­˜å‚¨ï¼‰"""
        current_time = time.time()
        
        if self._use_persistent and self._context_store:
            await self._context_store.add_message(
                user_id,
                role,
                content,
                group_id,
                message_id,
                timestamp=current_time,
                tool_calls=tool_calls,
                tool_call_id=tool_call_id,
            )
        else:
            key = self._get_context_key(user_id, group_id)
            msg = {"role": role, "content": content, "timestamp": current_time}
            if message_id:
                msg["message_id"] = str(message_id)
            if tool_calls and role == "assistant":
                msg["tool_calls"] = tool_calls
            if tool_call_id and role == "tool":
                msg["tool_call_id"] = str(tool_call_id)
            self._contexts[key].append(msg)
            max_history_messages = self.max_context * CONTEXT_HISTORY_MULTIPLIER
            if len(self._contexts[key]) > max_history_messages:
                self._contexts[key] = self._contexts[key][-max_history_messages:]

    # ==================== å…¼å®¹æ—§æµ‹è¯•çš„åŒæ­¥/ç§æœ‰ APIï¼ˆè–„å°è£…ï¼‰ ====================

    def _get_context(self, user_id: str, group_id: Optional[str] = None) -> List[Dict[str, Any]]:
        """å…¼å®¹æ—§æµ‹è¯•ï¼šåŒæ­¥è·å–å†…å­˜ä¸Šä¸‹æ–‡ã€‚

        æ³¨æ„ï¼šä»…åœ¨éæŒä¹…åŒ–æ¨¡å¼ä¸‹å¯é ï¼ˆæµ‹è¯•ç”¨ï¼‰ã€‚
        """
        if self._use_persistent:
            log.warning("_get_context åœ¨æŒä¹…åŒ–æ¨¡å¼ä¸‹ä¸å¯ç”¨ï¼Œæµ‹è¯•åº”ä½¿ç”¨ get_context")
            return []
        key = self._get_context_key(user_id, group_id)
        return self._contexts[key]

    def _add_to_context(
        self,
        user_id: str,
        role: str,
        content: Union[str, List[Dict[str, Any]]],
        group_id: Optional[str] = None,
        message_id: Optional[str] = None,
    ) -> None:
        """å…¼å®¹æ—§æµ‹è¯•ï¼šåŒæ­¥æ·»åŠ æ¶ˆæ¯åˆ°å†…å­˜ä¸Šä¸‹æ–‡ã€‚"""
        if self._use_persistent:
            log.warning("_add_to_context åœ¨æŒä¹…åŒ–æ¨¡å¼ä¸‹ä¸å¯ç”¨ï¼Œæµ‹è¯•åº”ä½¿ç”¨ add_message")
            return
        key = self._get_context_key(user_id, group_id)
        msg = {"role": role, "content": content, "timestamp": time.time()}
        if message_id:
            msg["message_id"] = str(message_id)
        self._contexts[key].append(msg)
        max_history_messages = self.max_context * CONTEXT_HISTORY_MULTIPLIER
        if len(self._contexts[key]) > max_history_messages:
            self._contexts[key] = self._contexts[key][-max_history_messages:]
    
    
    async def clear_context_async(self, user_id: str, group_id: Optional[str] = None):
        """å¼‚æ­¥æ¸…ç©ºä¸Šä¸‹æ–‡ï¼ˆæ”¯æŒæŒä¹…åŒ–å­˜å‚¨ï¼‰"""
        if self._use_persistent and self._context_store:
            await self._context_store.clear_context(user_id, group_id)
        else:
            key = self._get_context_key(user_id, group_id)
            self._contexts[key] = []
    
    def clear_context(self, user_id: str, group_id: Optional[str] = None):
        """åŒæ­¥æ¸…ç©ºä¸Šä¸‹æ–‡ï¼ˆä»…å†…å­˜æ¨¡å¼ï¼ŒæŒä¹…åŒ–æ¨¡å¼åº”ä½¿ç”¨ clear_context_asyncï¼‰"""
        if self._use_persistent:
            log.warning("åœ¨æŒä¹…åŒ–æ¨¡å¼ä¸‹ä½¿ç”¨åŒæ­¥ clear_context")
            return
        key = self._get_context_key(user_id, group_id)
        self._contexts[key] = []

    # ==================== å…¬å…±ä¸Šä¸‹æ–‡ç®¡ç† API ====================

    async def get_context(self, user_id: str, group_id: Optional[str] = None) -> List[Dict[str, Any]]:
        """è·å–ä¸Šä¸‹æ–‡æ¶ˆæ¯åˆ—è¡¨ï¼ˆå…¬å…± APIï¼‰
        
        Args:
            user_id: ç”¨æˆ· ID
            group_id: ç¾¤ç»„ IDï¼ˆå¯é€‰ï¼‰
            
        Returns:
            æ¶ˆæ¯å­—å…¸åˆ—è¡¨
        """
        return await self._get_context_async(user_id, group_id)

    async def add_message(
        self,
        user_id: str,
        role: str,
        content: Union[str, List[Dict[str, Any]]],
        group_id: Optional[str] = None,
        message_id: Optional[str] = None,
        timestamp: Optional[float] = None,
        tool_calls: Optional[List[Dict[str, Any]]] = None,
        tool_call_id: Optional[str] = None,
    ):
        """æ·»åŠ æ¶ˆæ¯åˆ°ä¸Šä¸‹æ–‡ï¼ˆå…¬å…± APIï¼‰
        
        Args:
            user_id: ç”¨æˆ· ID
            role: è§’è‰² ("user", "model", "system")
            content: æ¶ˆæ¯å†…å®¹
            group_id: ç¾¤ç»„ IDï¼ˆå¯é€‰ï¼‰
            message_id: æ¶ˆæ¯ IDï¼ˆå¯é€‰ï¼Œç”¨äºå»é‡ï¼‰
            timestamp: æ¶ˆæ¯æ—¶é—´æˆ³ï¼ˆå¯é€‰ï¼Œé»˜è®¤å½“å‰æ—¶é—´ï¼‰
        """
        current_time = timestamp if timestamp is not None else time.time()
        
        if self._use_persistent and self._context_store:
            await self._context_store.add_message(
                user_id,
                role,
                content,
                group_id,
                message_id,
                timestamp=current_time,
                tool_calls=tool_calls,
                tool_call_id=tool_call_id,
            )
        else:
            key = self._get_context_key(user_id, group_id)
            msg = {"role": role, "content": content, "timestamp": current_time}
            if message_id:
                msg["message_id"] = str(message_id)
            if tool_calls and role == "assistant":
                msg["tool_calls"] = tool_calls
            if tool_call_id and role == "tool":
                msg["tool_call_id"] = str(tool_call_id)
            self._contexts[key].append(msg)
            max_history_messages = self.max_context * CONTEXT_HISTORY_MULTIPLIER
            if len(self._contexts[key]) > max_history_messages:
                self._contexts[key] = self._contexts[key][-max_history_messages:]
    
    def _clean_thinking_markers(self, text: str) -> str:
        """æ¸…ç†æ¨¡å‹å›å¤ä¸­æ³„éœ²çš„æ€è€ƒè¿‡ç¨‹æ ‡è®°ï¼ˆå§”æ´¾åˆ°ç‹¬ç«‹æ¨¡å—ï¼‰ã€‚"""
        return clean_thinking_markers(text)

    # ==================== ç§æœ‰è¾…åŠ©æ–¹æ³•ï¼ˆchat æ‹†åˆ†ï¼‰ ====================

    def _prepare_chat_request(
        self,
        user_id: str,
        group_id: Optional[str],
        image_urls: Optional[List[str]],
    ) -> Tuple[str, float]:
        """ä¸º [`GeminiClient.chat()`](mika_chat_core/gemini_api.py:673) å‡†å¤‡è¯·æ±‚çº§ä¸Šä¸‹æ–‡ã€‚

        è¯¥æ–¹æ³•åªåšâ€œè¯·æ±‚çº§â€åˆå§‹åŒ–ï¼šç”Ÿæˆ request_idã€è®°å½•å¼€å§‹æ—¶é—´ã€è®¡æ•°ä¸èµ·å§‹æ—¥å¿—ã€‚
        è¡Œä¸ºéœ€ä¸æ‹†åˆ†å‰ä¿æŒä¸€è‡´ã€‚

        Returns:
            (request_id, start_time)
        """
        request_id = str(uuid.uuid4())[:UUID_SHORT_ID_LENGTH]
        start_time = time.time()
        metrics.requests_total += 1

        # ç»“æ„åŒ–æ—¥å¿—ï¼šè¯·æ±‚å¼€å§‹
        log.info(
            f"[req:{request_id}] å¼€å§‹å¤„ç†è¯·æ±‚ | "
            f"user={user_id} | group={group_id or 'private'} | "
            f"images={len(image_urls) if image_urls else 0}"
        )

        return request_id, start_time

    async def _log_context_diagnostics(self, user_id: str, group_id: Optional[str], request_id: str) -> None:
        """è¾“å‡ºä¸Šä¸‹æ–‡è¯Šæ–­æ—¥å¿—ï¼ˆé‡‡æ ·ï¼Œå¯è§‚æµ‹ï¼‰ã€‚"""
        trace_enabled = bool(getattr(plugin_config, "gemini_context_trace_enabled", False))
        if not trace_enabled:
            return

        sample_rate = float(getattr(plugin_config, "gemini_context_trace_sample_rate", 1.0) or 1.0)
        sample_rate = min(1.0, max(0.0, sample_rate))
        if sample_rate < 1.0 and random.random() > sample_rate:
            return

        history = await self._get_context_async(user_id, group_id)
        total_history_chars = sum(len(str(m.get("content", ""))) for m in history)
        log.info(
            f"[req:{request_id}] context_trace | phase=context_build | "
            f"history_count={len(history)} | "
            f"total_chars={total_history_chars} | "
            f"sample_rate={sample_rate:.2f}"
        )

        # æ‰“å°æœ€è¿‘ N æ¡å†å²æ¶ˆæ¯çš„æ‘˜è¦ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        if history:
            tail = history[-CONTEXT_DIAGNOSTIC_TAIL_COUNT:]
            for i, msg in enumerate(tail):
                role = msg.get("role", "unknown")
                content = str(msg.get("content", ""))
                # æˆªå–å‰ N å­—ç¬¦ï¼Œæ›¿æ¢æ¢è¡Œç¬¦
                content_preview = content[:HISTORY_MESSAGE_PREVIEW_CHARS].replace("\n", " ")
                if len(content) > HISTORY_MESSAGE_PREVIEW_CHARS:
                    content_preview += "..."
                log.debug(
                    f"[req:{request_id}] å†å²[{len(history)-len(tail)+i}] | "
                    f"role={role} | len={len(content)} | "
                    f"preview={content_preview}"
                )

    def _log_search_result_status(self, search_result: str, request_id: str) -> None:
        """è¾“å‡ºæœç´¢ç»“æœæ˜¯å¦æ³¨å…¥çš„æ—¥å¿—ï¼ˆä¿æŒåŸæ—¥å¿—å†…å®¹ï¼‰ã€‚"""
        if search_result:
            log.debug(f"[req:{request_id}] æœç´¢ç»“æœå·²æ³¨å…¥ | length={len(search_result)}")
        else:
            log.debug(f"[req:{request_id}] æ— æœç´¢ç»“æœæ³¨å…¥")

    def _coerce_pre_search_result(
        self,
        raw_result: Any,
        *,
        message: str,
        decision: str = "compat",
    ) -> PreSearchResult:
        """å…¼å®¹æ—§è¿”å›å€¼ï¼šå°† str/dict ç»Ÿä¸€æ”¶æ•›ä¸º PreSearchResultã€‚"""
        from .utils.search_engine import normalize_search_query

        bot_names = [
            getattr(plugin_config, "gemini_bot_display_name", "") or "",
            getattr(plugin_config, "gemini_master_name", "") or "",
        ]
        normalized_query = normalize_search_query(str(message or ""), bot_names=bot_names)

        if isinstance(raw_result, PreSearchResult):
            if not raw_result.normalized_query and normalized_query:
                raw_result.normalized_query = normalized_query
            return raw_result

        if isinstance(raw_result, dict):
            return PreSearchResult(
                search_result=str(raw_result.get("search_result") or ""),
                normalized_query=str(raw_result.get("normalized_query") or normalized_query),
                presearch_hit=bool(raw_result.get("presearch_hit")),
                allow_tool_refine=bool(raw_result.get("allow_tool_refine")),
                result_count=int(raw_result.get("result_count") or 0),
                refine_rounds_used=int(raw_result.get("refine_rounds_used") or 0),
                blocked_duplicate_total=int(raw_result.get("blocked_duplicate_total") or 0),
                decision=str(raw_result.get("decision") or decision),
            )

        search_result = str(raw_result or "")
        return PreSearchResult(
            search_result=search_result,
            normalized_query=normalized_query,
            presearch_hit=bool(search_result.strip()),
            allow_tool_refine=False,
            result_count=0,
            decision=decision,
        )

    def _log_search_decision(self, request_id: str, search_state: PreSearchResult, *, phase: str) -> None:
        """ç»Ÿä¸€æœç´¢ç¼–æ’æ—¥å¿—ã€‚"""
        log.info(
            f"[req:{request_id}] search_decision phase={phase} "
            f"presearch_hit={1 if search_state.presearch_hit else 0} "
            f"allow_refine={1 if search_state.allow_tool_refine else 0} "
            f"refine_used={search_state.refine_rounds_used} "
            f"blocked_duplicate={search_state.blocked_duplicate_total} "
            f"result_count={search_state.result_count}"
        )

    def _log_request_messages(self, messages: List[Dict[str, Any]], api_content: Any, request_id: str) -> None:
        """è¾“å‡ºå°†å‘é€ç»™æ¨¡å‹çš„æ¶ˆæ¯æ‘˜è¦æ—¥å¿—ï¼ˆDEBUGï¼‰ã€‚"""
        log.debug(f"[req:{request_id}] å‘é€æ¶ˆæ¯æ•°é‡: {len(messages)}")
        if isinstance(api_content, str) and len(api_content) > API_CONTENT_DEBUG_MIN_CHARS:
            log.debug(
                f"[req:{request_id}] APIæ¶ˆæ¯å†…å®¹ï¼ˆå‰{API_CONTENT_DEBUG_PREVIEW_CHARS}å­—ï¼‰:\n"
                f"{api_content[:API_CONTENT_DEBUG_PREVIEW_CHARS]}..."
            )

    async def _handle_server_error_retry(
        self,
        error: ServerError,
        message: str,
        user_id: str,
        group_id: Optional[str],
        image_urls: Optional[List[str]],
        enable_tools: bool,
        retry_count: int,
        message_id: Optional[str],
        system_injection: Optional[str],
        context_level: int,
        history_override: Optional[List[Dict[str, Any]]] = None,
        search_result: Optional[str] = None,
    ) -> Optional[str]:
        """å¤„ç†æœåŠ¡ç«¯é”™è¯¯çš„é‡è¯•é€»è¾‘ã€‚

        æ³¨æ„ï¼šæ­¤å¤„éœ€è¦ä¸¥æ ¼ä¿æŒæ‹†åˆ†å‰çš„è°ƒç”¨å‚æ•°ï¼ˆæ‹†åˆ†å‰ä¼šä¸¢å¤± message_id/system_injection/context_levelï¼‰ã€‚

        Returns:
            è‹¥è§¦å‘é‡è¯•åˆ™è¿”å›é€’å½’ chat çš„æœ€ç»ˆå›å¤ï¼›å¦åˆ™è¿”å› None è®©è°ƒç”¨æ–¹ç»§ç»­æŠ›å‡ºå¼‚å¸¸ã€‚
        """
        if retry_count > 0 and "will retry" in str(error.message):
            import asyncio

            wait_time = SERVER_ERROR_RETRY_BACKOFF_BASE ** (SERVER_ERROR_RETRY_EXPONENT_OFFSET - retry_count)
            await asyncio.sleep(wait_time)
            return await self.chat(
                message=message,
                user_id=user_id,
                group_id=group_id,
                image_urls=image_urls,
                enable_tools=enable_tools,
                retry_count=retry_count - 1,
                message_id=message_id,
                system_injection=system_injection,
                context_level=context_level,
                history_override=history_override,
                search_result_override=search_result,
            )
        return None

    async def _resolve_reply(
        self,
        messages: List[Dict[str, Any]],
        assistant_message: Dict[str, Any],
        tool_calls: List[Dict[str, Any]],
        api_key: str,
        group_id: Optional[str],
        request_id: str,
        enable_tools: bool,
        tools: Optional[List[Dict[str, Any]]] = None,
        search_state: Optional[PreSearchResult] = None,
    ) -> Tuple[str, List[Dict[str, Any]]]:
        """æ ¹æ®æ˜¯å¦å­˜åœ¨å·¥å…·è°ƒç”¨ï¼Œè§£ææœ€ç»ˆå›å¤æ–‡æœ¬ã€‚"""
        if tool_calls and enable_tools:
            result = await self._handle_tool_calls(
                messages,
                assistant_message,
                tool_calls,
                api_key,
                group_id,
                request_id,
                tools,
                search_state=search_state,
                return_trace=True,
            )
            if isinstance(result, ToolLoopResult):
                return result.reply, result.trace_messages
            return str(result), []
        return assistant_message.get("content") or "", []

    def _log_raw_model_reply(self, reply: str, request_id: str) -> None:
        """è¾“å‡ºæ¨¡å‹åŸå§‹å›å¤ï¼ˆè°ƒè¯•ç”¨ï¼‰ã€‚"""
        log.debug(
            f"[req:{request_id}] æ¨¡å‹åŸå§‹å›å¤ï¼ˆå‰{RAW_MODEL_REPLY_PREVIEW_CHARS}å­—ï¼‰:\n"
            f"{reply[:RAW_MODEL_REPLY_PREVIEW_CHARS]}..."
            if len(reply) > RAW_MODEL_REPLY_PREVIEW_CHARS
            else f"[req:{request_id}] æ¨¡å‹åŸå§‹å›å¤:\n{reply}"
        )

    def _process_response(self, reply: str, request_id: str) -> str:
        """å¤„ç†å“åº”æ–‡æœ¬ï¼ˆæ¸…ç†æ€è€ƒæ ‡è®°/è§’è‰²æ ‡ç­¾/Markdownæ ¼å¼/ç©ºç™½ï¼‰ã€‚"""
        original_len = len(reply)
        reply = self._clean_thinking_markers(reply)

        import re

        # ==================== Markdown/LaTeX æ ¼å¼æ¸…ç† ====================
        # åœ¨æ‰‹æœº QQ ä¸Šè¿™äº›æ ¼å¼ä¼šæ˜¾ç¤ºä¸ºåŸå§‹ç¬¦å·ï¼Œéœ€è¦æ¸…ç†

        # 1. æ¸…ç†ç²—ä½“ï¼š**æ–‡å­—** æˆ– __æ–‡å­—__ â†’ æ–‡å­—
        reply = re.sub(r"\*\*(.+?)\*\*", r"\1", reply)
        reply = re.sub(r"__(.+?)__", r"\1", reply)

        # 2. æ¸…ç†æ–œä½“ï¼š*æ–‡å­—* æˆ– _æ–‡å­—_ â†’ æ–‡å­—ï¼ˆæ³¨æ„é¿å…ä¸åˆ—è¡¨ç¬¦å·å†²çªï¼‰
        # åªåŒ¹é…éç©ºæ ¼å¼€å¤´çš„ *æ–‡å­—*ï¼Œé¿å…è¯¯åˆ åˆ—è¡¨é¡¹
        reply = re.sub(r"(?<!\*)\*([^\s*][^*]*?)\*(?!\*)", r"\1", reply)
        reply = re.sub(r"(?<!_)_([^\s_][^_]*?)_(?!_)", r"\1", reply)

        # 3. æ¸…ç†è¡Œå†…ä»£ç ï¼š`ä»£ç ` â†’ ä»£ç 
        reply = re.sub(r"`([^`]+)`", r"\1", reply)

        # 4. æ¸…ç†ä»£ç å—ï¼š```lang\nä»£ç \n``` â†’ ä»£ç 
        reply = re.sub(r"```(?:\w+)?\n?(.*?)\n?```", r"\1", reply, flags=re.DOTALL)

        # 5. æ¸…ç†ç¼–å·åˆ—è¡¨å¼€å¤´ï¼šå°† "1. " "2. " ç­‰è½¬ä¸º "1ã€" "2ã€" æˆ–ç›´æ¥å»æ‰
        # ä¿ç•™æ•°å­—ä½†ç”¨ä¸­æ–‡é¡¿å·æ›¿ä»£ç‚¹+ç©ºæ ¼
        reply = re.sub(r"(?m)^(\d+)\.\s+", r"\1ã€", reply)

        # 6. æ¸…ç†æ— åºåˆ—è¡¨ç¬¦å·ï¼š- æˆ– * å¼€å¤´ï¼ˆè¡Œé¦–ï¼‰â†’ æ›¿æ¢ä¸ºä¸­æ–‡åœ†ç‚¹ï¼ˆä¿ç•™è§†è§‰åˆ†éš”ï¼‰
        reply = re.sub(r"(?m)^[\-\*]\s+", "Â· ", reply)

        # 7. æ¸…ç†æ ‡é¢˜ï¼š# ## ### ç­‰ â†’ æ›¿æ¢ä¸ºã€ã€‘åŒ…è£¹ï¼ˆä¿ç•™å±‚æ¬¡æ„Ÿï¼‰
        reply = re.sub(r"(?m)^#{1,6}\s*(.+)$", r"ã€\1ã€‘", reply)

        # 8. æ¸…ç†å¼•ç”¨ï¼š> å¼€å¤´ â†’ æ›¿æ¢ä¸ºã€Œã€åŒ…è£¹ï¼ˆåŒºåˆ†å¼•ç”¨å†…å®¹ï¼‰
        reply = re.sub(r"(?m)^>\s*(.+)$", r"ã€Œ\1ã€", reply)

        # 9. æ¸…ç† LaTeX å…¬å¼ï¼š$å…¬å¼$ æˆ– $$å…¬å¼$$ â†’ ä¿ç•™å…¬å¼æ–‡æœ¬
        reply = re.sub(r"\$\$(.+?)\$\$", r"\1", reply, flags=re.DOTALL)
        reply = re.sub(r"\$([^$]+)\$", r"\1", reply)

        # 10. æ¸…ç†é“¾æ¥ï¼š[æ–‡å­—](url) â†’ æ–‡å­—
        reply = re.sub(r"\[([^\]]+)\]\([^)]+\)", r"\1", reply)

        # ==================== ç”¨æˆ·æ˜µç§°æ ‡ç­¾æ¸…ç†ï¼ˆå¢å¼ºç‰ˆï¼‰ ====================
        # å³ä½¿ Prompt ç¦æ­¢ï¼Œæ¨¡å‹å¶å°”è¿˜æ˜¯ä¼šå¤è¯» `[æ˜µç§°(QQ)]`ã€‚è¿™é‡Œåœ¨ä»£ç å±‚å¼ºåˆ¶ç§»é™¤ã€‚
        # æ³¨æ„ï¼šä¸æ¸…ç†ä¸­æ–‡æ–¹æ‹¬å·ã€ã€‘ï¼Œå¦‚ã€è‡ªä¸»å›å¤ã€‘

        # 0. [æ–°å¢] å…ˆç§»é™¤é›¶å®½å­—ç¬¦ï¼Œä¾¿äºåç»­æ­£åˆ™åŒ¹é…
        # é›¶å®½å­—ç¬¦åŒ…æ‹¬ï¼šU+200B (é›¶å®½ç©ºæ ¼), U+200C (é›¶å®½éè¿æ¥ç¬¦), U+200D (é›¶å®½è¿æ¥ç¬¦),
        # U+2060 (è¯ç»„è¿æ¥ç¬¦), U+FEFF (BOM), U+202A-U+202E (åŒå‘æ§åˆ¶å­—ç¬¦) ç­‰
        zero_width_pattern = r"[\u200B-\u200F\u2060-\u206F\uFEFF\u202A-\u202E]"
        reply_cleaned = re.sub(zero_width_pattern, "", reply)

        # 1. åŒ¹é… [æ˜µç§°(QQå·)]: æˆ– [æ˜µç§°(QQå·)] æ ¼å¼ï¼ˆä»»æ„ä½ç½®ï¼Œä¸ä»…é™è¡Œé¦–ï¼‰
        reply_cleaned = re.sub(r"\[[^\]]*?\(\d+\)\]:?\s*", "", reply_cleaned)

        # 2. åŒ¹é…å¸¸è§è§’è‰²æ ‡ç­¾ï¼š[Mika]: [Sensei]: [â˜…Sensei]: [User]: [æœªèŠ±]: ç­‰ï¼ˆå¸¦æˆ–ä¸å¸¦å†’å·ï¼‰
        reply_cleaned = re.sub(
            r"\[(Mika|Sensei|â˜…Sensei|â­Sensei|User|æœªèŠ±|åœ£å›­æœªèŠ±)\]:?\s*",
            "",
            reply_cleaned,
            flags=re.IGNORECASE,
        )

        # 3. åŒ¹é…è¡Œé¦–çš„ä»»æ„çŸ­æ ‡ç­¾ [xxx]: æ ¼å¼ï¼ˆ1-20å­—ç¬¦ï¼Œå¸¦å†’å·ï¼Œé˜²æ­¢è¯¯åˆ é•¿å†…å®¹ï¼‰
        reply_cleaned = re.sub(r"(?m)^\[[^\]]{1,20}\]:\s*", "", reply_cleaned)

        # 4. [æ–°å¢] åŒ¹é…ä»»æ„ä½ç½®çš„ç”¨æˆ·æ˜µç§°æ ‡ç­¾ [xxx] æ ¼å¼ï¼ˆä¸å¸¦å†’å·ï¼‰
        #    ç‰¹å¾ï¼šæ–¹æ‹¬å·å†…åŒ…å«ç‰¹æ®Šç¬¦å·ï¼ˆå¦‚â™¡â˜…â˜†â­ç­‰ï¼‰æˆ–çº¯æ•°å­—ï¼Œé•¿åº¦1-30
        #    æ’é™¤ï¼šçº¯ä¸­æ–‡çŸ­æ ‡ç­¾å¦‚ [ç¬‘] [å“­] [å›¾ç‰‡] ç­‰å¸¸è§è¡¨æƒ…/å ä½ç¬¦
        #    ç­–ç•¥ï¼šåŒ¹é…å«ç‰¹æ®Šç¬¦å·çš„æ ‡ç­¾ï¼Œæˆ–åŒ¹é…è¡Œé¦–çš„çŸ­æ ‡ç­¾
        reply_cleaned = re.sub(
            r"\[(?=[^\]]*[â™¡â˜…â˜†â­â™ªâ™«âœ¨ğŸ’•ğŸµ])[^\]]{1,30}\]",  # å«è£…é¥°ç¬¦å·çš„æ ‡ç­¾
            "",
            reply_cleaned,
        )

        # 5. åŒ¹é…æ—¶é—´æ„ŸçŸ¥æ ‡ç­¾ï¼ˆæ‰©å±•ç‰ˆï¼‰ï¼š
        #    - åŸºç¡€æ—¶é—´è¯ï¼š[ç°åœ¨] [åˆšæ‰] [åˆšåˆš] [ä¹‹å‰] [ç¨å] [é©¬ä¸Š]
        #    - ç›¸å¯¹æ—¶é—´ï¼š[å‡ åˆ†é’Ÿå‰] [åŠå°æ—¶å‰] [çº¦Xå°æ—¶å‰] [Xå¤©å‰] [1-2å°æ—¶å‰]
        #    - æ—¥æ–‡/æ··åˆæ ¼å¼ï¼š[ãƒŸã‚«ã¡ã‚ƒã‚“ã®æ²»ç™‚] ç­‰å¸¦"ã®"çš„æ ‡ç­¾
        reply_cleaned = re.sub(
            r"\[(?:"
            r"ç°åœ¨|åˆšæ‰|åˆšåˆš|ä¹‹å‰|ç¨å|é©¬ä¸Š|"  # åŸºç¡€æ—¶é—´è¯
            r"å‡ åˆ†é’Ÿå‰|åŠå°æ—¶å‰|çº¦?\d+(?:å°æ—¶|å¤©|åˆ†é’Ÿ)å‰|1-2å°æ—¶å‰|"  # ç›¸å¯¹æ—¶é—´
            r"[^\]]{1,15}ã®[^\]]{1,15}"  # æ—¥æ–‡"ã®"æ ¼å¼æ ‡ç­¾
            r")\]\s*",
            "",
            reply_cleaned,
        )

        # 6. åŒ¹é…å›å¤å¼€å¤´çš„å•ç‹¬çŸ­æ ‡ç­¾ [xxx] æ ¼å¼ï¼ˆä¸å¸¦å†’å·ï¼Œ1-10å­—ç¬¦ï¼Œä»…è¡Œé¦–ï¼‰
        #    è¿™åŒ¹é…å¦‚ [æ²»ç–—] [ä¼‘æ¯] ç­‰å•ç‹¬å‡ºç°åœ¨å¼€å¤´çš„æ ‡ç­¾
        reply_cleaned = re.sub(r"^(?:\[[^\]]{1,10}\]\s*)+", "", reply_cleaned)

        # 7. [æ–°å¢] æ¸…ç†ç³»ç»Ÿæ ‡ç­¾ï¼š[æœç´¢ä¸­] [æ€è€ƒä¸­] [ç”Ÿæˆä¸­] ç­‰
        reply_cleaned = re.sub(r"\[(?:æœç´¢ä¸­|æ€è€ƒä¸­|ç”Ÿæˆä¸­|åŠ è½½ä¸­|å¤„ç†ä¸­)\]", "", reply_cleaned)

        reply = reply_cleaned

        # 7. æ¸…ç†é¦–å°¾ç©ºç™½ + é¦–è¡Œå‰å¯¼ç©ºç™½ï¼ˆä¿®å¤è½¬å‘æ¶ˆæ¯ä¸­çš„ç¼©è¿›é—®é¢˜ï¼‰
        reply = reply.strip()
        # ç§»é™¤é¦–è¡Œçš„å‰å¯¼ç©ºæ ¼/åˆ¶è¡¨ç¬¦ï¼ˆé˜²æ­¢å‡ºç°ä¸å¯¹é½çš„æƒ…å†µï¼‰
        reply = re.sub(r"^[\s\u3000]+", "", reply)  # åŒ…æ‹¬ä¸­æ–‡å…¨è§’ç©ºæ ¼

        if len(reply) != original_len:
            log.debug(
                f"[req:{request_id}] å·²æ¸…ç†æ ¼å¼/æ ‡ç­¾ | åŸé•¿åº¦={original_len} | æ¸…ç†å={len(reply)}"
            )

        return reply

    async def _handle_empty_reply_retry(
        self,
        request_id: str,
        start_time: float,
        message: str,
        user_id: str,
        group_id: Optional[str],
        image_urls: Optional[List[str]],
        enable_tools: bool,
        retry_count: int,
        message_id: Optional[str],
        system_injection: Optional[str],
        context_level: int,
        history_override: Optional[List[Dict[str, Any]]],
        search_result: str,
    ) -> Optional[str]:
        """å¤„ç†ç©ºå›å¤çš„â€œä¸Šä¸‹æ–‡é™çº§â€é‡è¯•é€»è¾‘ï¼ˆä¿æŒæ‹†åˆ†å‰è¡Œä¸ºä¸å˜ï¼‰ã€‚

        Returns:
            è‹¥è§¦å‘é™çº§é‡è¯•åˆ™è¿”å›é€’å½’ chat çš„æœ€ç»ˆå›å¤ï¼›å¦åˆ™è¿”å› Noneã€‚
        """
        total_elapsed = time.time() - start_time
        log.warning(
            f"[req:{request_id}] ç©ºå›å¤ (retry={retry_count}, context_level={context_level}) | "
            f"total_time={total_elapsed:.2f}s"
        )

        if not bool(getattr(plugin_config, "gemini_empty_reply_context_degrade_enabled", False)):
            log.warning(f"[req:{request_id}] ç©ºå›å¤ä¸è§¦å‘ä¸šåŠ¡çº§ä¸Šä¸‹æ–‡é™çº§ï¼ˆé…ç½®å…³é—­ï¼‰")
            return None

        max_degrade_level = int(
            getattr(
                plugin_config,
                "gemini_empty_reply_context_degrade_max_level",
                MAX_CONTEXT_DEGRADATION_LEVEL,
            )
            or MAX_CONTEXT_DEGRADATION_LEVEL
        )
        if max_degrade_level < 0:
            max_degrade_level = 0

        # [æ™ºèƒ½é™çº§] ç©ºå›å¤æ—¶é€çº§å‡å°‘ä¸Šä¸‹æ–‡
        # Level 0 -> Level 1 (20æ¡) -> Level 2 (5æ¡) -> æ”¾å¼ƒ
        next_context_level = context_level + 1
        if next_context_level <= max_degrade_level:
            import asyncio

            wait_time = EMPTY_REPLY_RETRY_DELAY_SECONDS
            await asyncio.sleep(wait_time)
            metrics.api_empty_reply_reason_total["context_degrade"] = (
                int(metrics.api_empty_reply_reason_total.get("context_degrade", 0) or 0) + 1
            )
            log.warning(
                f"[req:{request_id}] è§¦å‘ä¸Šä¸‹æ–‡é™çº§é‡è¯• | "
                f"Level {context_level} -> Level {next_context_level} (max={max_degrade_level})"
            )
            return await self.chat(
                message=message,
                user_id=user_id,
                group_id=group_id,
                image_urls=image_urls,
                enable_tools=enable_tools,
                retry_count=retry_count,  # ä¿æŒé‡è¯•æ¬¡æ•°
                message_id=message_id,
                system_injection=system_injection,
                context_level=next_context_level,  # æå‡é™çº§å±‚çº§
                history_override=history_override,
                search_result_override=search_result,
            )
        return None

    def _log_request_success(self, request_id: str, start_time: float, reply: str, tool_calls: List[Dict[str, Any]]) -> None:
        """è¾“å‡ºè¯·æ±‚æˆåŠŸçš„ç»“æ„åŒ–æ—¥å¿—ï¼ˆä¿æŒæ‹†åˆ†å‰æ—¥å¿—å†…å®¹ï¼‰ã€‚"""
        total_elapsed = time.time() - start_time
        tool_info = f" | tools_called={len(tool_calls)}" if tool_calls else ""
        log.success(
            f"[req:{request_id}] è¯·æ±‚å®Œæˆ | "
            f"reply_len={len(reply)}{tool_info} | "
            f"total_time={total_elapsed:.2f}s"
        )
    
    async def _pre_search(
        self,
        message: str,
        enable_tools: bool,
        request_id: str,
        user_id: str = None,
        group_id: str = None
    ) -> PreSearchResult:
        """é¢„æ‰§è¡Œæœç´¢ï¼ˆå§”æ´¾åˆ°ç‹¬ç«‹æ¨¡å—ï¼Œä¿æŒè¡Œä¸ºä¸å˜ï¼‰ã€‚"""
        raw_result = await pre_search(
            message,
            enable_tools=enable_tools,
            request_id=request_id,
            tool_handlers=self._tool_handlers,
            enable_smart_search=self._enable_smart_search,
            get_context_async=self._get_context_async,
            get_api_key=self._get_api_key,
            base_url=self.base_url,
            user_id=user_id,
            group_id=group_id,
            return_meta=True,
        )
        return self._coerce_pre_search_result(raw_result, message=message, decision="presearch")
    
    async def _build_messages(
        self,
        message: str,
        user_id: str,
        group_id: Optional[str],
        image_urls: Optional[List[str]],
        search_result: str,
        enable_tools: bool = True,
        system_injection: Optional[str] = None,
        context_level: int = 0,  # [æ–°å¢] ä¸Šä¸‹æ–‡é™çº§å±‚çº§
        history_override: Optional[List[Dict[str, Any]]] = None,
    ) -> tuple:
        """æ„å»ºæ¶ˆæ¯å†å²ä¸è¯·æ±‚ä½“ï¼ˆå§”æ´¾åˆ°ç‹¬ç«‹æ¨¡å—ï¼Œä¿æŒè¡Œä¸ºä¸å˜ï¼‰ã€‚"""
        result = await build_messages(
            message,
            user_id=user_id,
            group_id=group_id,
            image_urls=image_urls,
            search_result=search_result,
            model=self.model,
            system_prompt=self.system_prompt,
            available_tools=self.AVAILABLE_TOOLS,
            system_injection=system_injection,
            context_level=context_level,
            history_override=history_override,
            get_context_async=self._get_context_async,
            use_persistent=self._use_persistent,
            context_store=self._context_store,
            has_image_processor=HAS_IMAGE_PROCESSOR,
            get_image_processor=get_image_processor,
            has_user_profile=HAS_USER_PROFILE,
            get_user_profile_store=get_user_profile_store,
            enable_tools=enable_tools,
        )

        return (
            result.messages,
            result.original_content,
            result.api_content,
            result.request_body,
        )
    
    async def _send_api_request(
        self,
        request_body: Dict[str, Any],
        request_id: str,
        retry_count: int,
        message: str,
        user_id: str,
        group_id: Optional[str],
        image_urls: Optional[List[str]],
        enable_tools: bool
    ) -> tuple:
        """å‘é€ API è¯·æ±‚å¹¶å¤„ç†å“åº”ï¼ˆå§”æ´¾åˆ°ç‹¬ç«‹æ¨¡å—ï¼Œä¿æŒè¡Œä¸ºä¸å˜ï¼‰ã€‚"""
        client = await self._get_client()
        current_api_key = self._get_api_key()
        try:
            return await send_api_request(
                http_client=client,
                request_body=request_body,
                request_id=request_id,
                retry_count=retry_count,
                api_key=current_api_key,
                base_url=self.base_url,
                model=self.model,
            )
        except RateLimitError as e:
            self._mark_key_rate_limited(current_api_key, e.retry_after)
            raise
    
    async def _handle_tool_calls(
        self,
        messages: List[Dict[str, Any]],
        assistant_message: Dict[str, Any],
        tool_calls: List[Dict[str, Any]],
        api_key: str,
        group_id: Optional[str],
        request_id: str,
        tools: Optional[List[Dict[str, Any]]] = None,
        search_state: Optional[PreSearchResult] = None,
        return_trace: bool = False,
    ) -> str | ToolLoopResult:
        """å¤„ç†å·¥å…·è°ƒç”¨ï¼ˆå§”æ´¾åˆ°ç‹¬ç«‹æ¨¡å—ï¼Œä¿æŒè¡Œä¸ºä¸å˜ï¼‰ã€‚"""
        client = await self._get_client()
        return await handle_tool_calls(
            messages=messages,
            assistant_message=assistant_message,
            tool_calls=tool_calls,
            api_key=api_key,
            group_id=group_id,
            request_id=request_id,
            tool_handlers=self._tool_handlers,
            model=self.model,
            base_url=self.base_url,
            http_client=client,
            tools=tools,
            search_state=search_state,
            return_trace=return_trace,
        )
    
    async def _update_context(
        self,
        user_id: str,
        group_id: Optional[str],
        current_content: Union[str, List[Dict[str, Any]]],
        reply: str,
        user_message_id: Optional[str] = None,
        tool_trace: Optional[List[Dict[str, Any]]] = None,
    ) -> None:
        """
        æ›´æ–°ä¸Šä¸‹æ–‡ï¼ˆä¿å­˜å¯¹è¯å†å²ï¼‰
        
        Args:
            user_id: ç”¨æˆ· ID
            group_id: ç¾¤ç»„ IDï¼ˆå¯é€‰ï¼‰
            current_content: å½“å‰ç”¨æˆ·æ¶ˆæ¯å†…å®¹
            reply: åŠ©æ‰‹å›å¤
            user_message_id: ç”¨æˆ·æ¶ˆæ¯ IDï¼ˆç”¨äºå»é‡ï¼‰
        """
        await self._add_to_context_async(user_id, "user", current_content, group_id, user_message_id)

        for trace_msg in tool_trace or []:
            if not isinstance(trace_msg, dict):
                continue
            trace_role = str(trace_msg.get("role") or "").strip().lower()
            if trace_role not in {"assistant", "tool"}:
                continue
            trace_content: Union[str, List[Dict[str, Any]]] = trace_msg.get("content") or ""
            tool_calls = trace_msg.get("tool_calls")
            tool_call_id = trace_msg.get("tool_call_id")
            if (
                trace_role == "assistant"
                and not tool_calls
                and isinstance(trace_content, str)
                and trace_content.strip() == reply.strip()
            ):
                continue
            await self._add_to_context_async(
                user_id,
                trace_role,
                trace_content,
                group_id,
                tool_calls=tool_calls if isinstance(tool_calls, list) else None,
                tool_call_id=str(tool_call_id) if tool_call_id else None,
            )
        
        # [æ”¹è¿›] ç¾¤èŠä¸­ä¸º Bot å›å¤æ·»åŠ è§’è‰²æ ‡ç­¾ï¼Œå¸®åŠ© LLM åŒºåˆ†è¯´è¯è€…
        # æ ¼å¼: [Mika]: xxxï¼Œä¸ç”¨æˆ·æ¶ˆæ¯çš„ [æ˜µç§°(QQ)]: xxx æ ¼å¼ä¸€è‡´
        if group_id:
            formatted_reply = f"[{self._character_name}]: {reply}"
        else:
            formatted_reply = reply
        
        await self._add_to_context_async(user_id, "assistant", formatted_reply, group_id)
    
    def _handle_error(
        self,
        error: Exception,
        request_id: str,
        start_time: float
    ) -> str:
        """
        ç»Ÿä¸€é”™è¯¯å¤„ç†
        
        Args:
            error: å¼‚å¸¸å¯¹è±¡
            request_id: è¯·æ±‚è¿½è¸ª ID
            start_time: è¯·æ±‚å¼€å§‹æ—¶é—´
            
        Returns:
            ç”¨æˆ·å‹å¥½çš„é”™è¯¯æ¶ˆæ¯
        """
        total_elapsed = time.time() - start_time
        
        if isinstance(error, httpx.TimeoutException):
            log.error(f"[req:{request_id}] è¶…æ—¶é”™è¯¯ | error={str(error)} | total_time={total_elapsed:.2f}s")
            return self._get_error_message("timeout")
        
        if isinstance(error, RateLimitError):
            log.warning(f"[req:{request_id}] é™æµé”™è¯¯ | retry_after={error.retry_after}s | total_time={total_elapsed:.2f}s")
            return self._get_error_message("rate_limit")
        
        if isinstance(error, AuthenticationError):
            log.error(f"[req:{request_id}] è®¤è¯é”™è¯¯ | status={error.status_code} | total_time={total_elapsed:.2f}s")
            return self._get_error_message("auth_error")
        
        if isinstance(error, ServerError):
            log.error(f"[req:{request_id}] æœåŠ¡ç«¯é”™è¯¯ | status={error.status_code} | total_time={total_elapsed:.2f}s")
            return self._get_error_message("server_error")
        
        if isinstance(error, GeminiAPIError):
            log.error(f"[req:{request_id}] API é”™è¯¯ | status={error.status_code} | total_time={total_elapsed:.2f}s")
            # æ£€æŸ¥æ˜¯å¦ä¸ºå†…å®¹è¿‡æ»¤é”™è¯¯
            if "content filtered" in str(error.message).lower():
                return self._get_error_message("content_filter")
            return self._get_error_message("api_error")

        if isinstance(error, (httpx.ConnectError, httpx.NetworkError)):
            # ç½‘ç»œè¿æ¥é”™è¯¯ï¼ˆå¦‚ DNS è§£æå¤±è´¥ã€è¿æ¥è¢«é‡ç½®ç­‰ï¼‰
            log.warning(f"[req:{request_id}] ç½‘ç»œè¿æ¥é”™è¯¯ | error={str(error)} | total_time={total_elapsed:.2f}s")
            return self._get_error_message("timeout")  # ç»Ÿä¸€ä½œä¸ºç½‘ç»œè¶…æ—¶å¤„ç†
        
        # RemoteProtocolError åœ¨ httpx å†…éƒ¨æ¨¡å—ï¼Œéœ€è¦æ£€æŸ¥ç±»å
        error_class_name = type(error).__name__
        if error_class_name == "RemoteProtocolError" or "Server disconnected" in str(error):
            # è¿œç¨‹åè®®é”™è¯¯ï¼šæœåŠ¡å™¨åœ¨å‘é€å“åº”å‰æ–­å¼€è¿æ¥
            # å¸¸è§åŸå› ï¼šä¸­è½¬ä»£ç†è¶…æ—¶ï¼ˆé€šå¸¸60ç§’ï¼‰ã€HTTP/2è¿æ¥è¢«æ„å¤–ç»ˆæ­¢
            log.warning(
                f"[req:{request_id}] è¿œç¨‹åè®®é”™è¯¯ï¼ˆæœåŠ¡å™¨æ–­å¼€ï¼‰ | "
                f"error={str(error)} | total_time={total_elapsed:.2f}s | "
                f"hint=å¯èƒ½æ˜¯ä¸­è½¬æœåŠ¡å™¨è¶…æ—¶ï¼Œå»ºè®®æ£€æŸ¥ä»£ç†é…ç½®æˆ–å‡å°‘è¯·æ±‚å¤æ‚åº¦"
            )
            return self._get_error_message("timeout")  # å¯¹ç”¨æˆ·è¡¨ç°ä¸ºè¶…æ—¶
            
        # æœªçŸ¥é”™è¯¯
        log.error(
            f"[req:{request_id}] æœªçŸ¥é”™è¯¯ | error={type(error).__name__}: {str(error)} | total_time={total_elapsed:.2f}s",
            exc_info=True
        )
        if hasattr(error, 'response') and error.response:
            log.error(
                f"[req:{request_id}] Response Body: {error.response.text[:ERROR_RESPONSE_BODY_PREVIEW_CHARS]}"
            )
        return self._get_error_message("unknown")
    
    # ==================== ä¸»å¯¹è¯æ–¹æ³• ====================
    
    async def chat(
        self,
        message: str,
        user_id: str,
        group_id: Optional[str] = None,
        image_urls: Optional[List[str]] = None,
        enable_tools: bool = True,
        retry_count: int = DEFAULT_CHAT_RETRY_COUNT,  # [ä¼˜åŒ–] æé«˜é»˜è®¤é‡è¯•æ¬¡æ•°
        message_id: Optional[str] = None,
        system_injection: Optional[str] = None,  # System çº§æ³¨å…¥ï¼ˆç”¨äºä¸»åŠ¨å‘è¨€ç­‰åœºæ™¯ï¼‰
        context_level: int = 0,  # [æ–°å¢] ä¸Šä¸‹æ–‡é™çº§å±‚çº§ (0=å®Œæ•´, 1=æˆªæ–­, 2=æœ€å°)
        history_override: Optional[List[Dict[str, Any]]] = None,
        search_result_override: Optional[str] = None,  # å†…éƒ¨å‚æ•°ï¼šå¤ç”¨é¦–è½®æœç´¢ç»“æœï¼Œé¿å…é‡è¯•é‡å¤åˆ†ç±»/æœç´¢
    ) -> str:
        """
        å‘é€æ¶ˆæ¯ï¼ˆæ”¯æŒ OpenAI æ ¼å¼ï¼‰
        
        è¿™æ˜¯å¯¹è¯çš„ä¸»å…¥å£æ–¹æ³•ï¼Œåè°ƒå„ä¸ªå­æ­¥éª¤ï¼š
        1. é¢„æ‰§è¡Œæœç´¢ï¼ˆå¯é€‰ï¼‰
        2. æ„å»ºæ¶ˆæ¯å†å²
        3. å‘é€ API è¯·æ±‚
        4. å¤„ç†å·¥å…·è°ƒç”¨ï¼ˆå¦‚æœ‰ï¼‰
        5. æ›´æ–°ä¸Šä¸‹æ–‡
        
        æ™ºèƒ½é™çº§é‡è¯•æœºåˆ¶ï¼š
        - Level 0: å®Œæ•´ä¸Šä¸‹æ–‡ (max_context)
        - Level 1: æˆªæ–­ä¸Šä¸‹æ–‡ (20æ¡)
        - Level 2: æœ€å°ä¸Šä¸‹æ–‡ (5æ¡)
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            user_id: ç”¨æˆ· ID
            group_id: ç¾¤ç»„ IDï¼ˆå¯é€‰ï¼Œç§èŠæ—¶ä¸º Noneï¼‰
            image_urls: å›¾ç‰‡ URL åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            enable_tools: æ˜¯å¦å¯ç”¨å·¥å…·ï¼ˆé»˜è®¤ Trueï¼‰
            retry_count: é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤ 2ï¼‰
            message_id: æ¶ˆæ¯ IDï¼ˆå¯é€‰ï¼Œç”¨äºä¸Šä¸‹æ–‡å»é‡ï¼‰
            system_injection: System çº§æ³¨å…¥å†…å®¹ï¼ˆå¯é€‰ï¼Œç”¨äºä¸»åŠ¨å‘è¨€åœºæ™¯ç­‰ï¼‰
            context_level: ä¸Šä¸‹æ–‡é™çº§å±‚çº§ (0=å®Œæ•´, 1=æˆªæ–­20æ¡, 2=æœ€å°5æ¡)
            history_override: è¦†ç›–ä¸Šä¸‹æ–‡å†å²ï¼ˆä»…å½±å“æœ¬æ¬¡è¯·æ±‚æ„å»º messagesï¼›None=ä½¿ç”¨å­˜å‚¨çš„å†å²ï¼‰
            search_result_override: å¤ç”¨æœç´¢ç»“æœï¼ˆå†…éƒ¨é‡è¯•å‚æ•°ï¼Œå¤–éƒ¨æ— éœ€ä¼ ï¼‰
            
        Returns:
            åŠ©æ‰‹å›å¤å†…å®¹
        """
        request_id, start_time = self._prepare_chat_request(user_id, group_id, image_urls)
        
        # å…¼å®¹ tests å¯¹â€œä¸Šä¸‹æ–‡é™çº§é‡è¯•â€å®ç°ç»†èŠ‚çš„æ–­è¨€ï¼š
        # tests ä¼šç”¨ `inspect.getsource(client.chat)` æ£€æŸ¥è¯¥å˜é‡åæ˜¯å¦å­˜åœ¨ã€‚
        # å®é™…é™çº§é€»è¾‘åœ¨ `_handle_empty_reply_retry()` ä¸­å®ç°ã€‚
        next_context_level = context_level + 1

        try:
            # ===== ä¸Šä¸‹æ–‡è¯Šæ–­æ—¥å¿— =====
            await self._log_context_diagnostics(user_id, group_id, request_id)
            
            # 1. é¢„å¤„ç†ï¼šæœç´¢å¢å¼ºï¼ˆé‡è¯•æ—¶å¯å¤ç”¨é¦–è½®ç»“æœï¼Œé¿å…é‡å¤è§¦å‘åˆ†ç±»å™¨/æœç´¢ï¼‰
            if search_result_override is None:
                search_state = self._coerce_pre_search_result(
                    await self._pre_search(
                        message, enable_tools, request_id,
                        user_id=user_id, group_id=group_id
                    ),
                    message=message,
                    decision="presearch",
                )
                search_result = search_state.search_result
            else:
                search_state = self._coerce_pre_search_result(
                    search_result_override,
                    message=message,
                    decision="override",
                )
                search_result = search_state.search_result
                log.info(
                    f"[req:{request_id}] å¤ç”¨é¦–è½®æœç´¢åˆ¤å®šç»“æœï¼Œè·³è¿‡é‡å¤åˆ†ç±»/æœç´¢ | "
                    f"search_injected={'yes' if bool(search_result) else 'no'}"
                )

            # è¾“å‡ºæœç´¢ç»“æœçŠ¶æ€
            self._log_search_result_status(search_result, request_id)
            self._log_search_decision(request_id, search_state, phase="pre_send")

            # 2. æ„å»ºè¯·æ±‚ï¼ˆæ³¨æ„ï¼šç°åœ¨è¿”å› 4 ä¸ªå€¼ï¼‰
            # - messages: å®Œæ•´æ¶ˆæ¯åˆ—è¡¨
            # - original_content: åŸå§‹ç”¨æˆ·æ¶ˆæ¯ï¼ˆç”¨äºä¿å­˜å†å²ï¼‰
            # - api_content: API è¯·æ±‚æ¶ˆæ¯ï¼ˆå¯èƒ½å«æœç´¢ç»“æœï¼‰
            # - request_body: è¯·æ±‚ä½“
            messages, original_content, api_content, request_body = await self._build_messages(
                message,
                user_id,
                group_id,
                image_urls,
                search_result,
                enable_tools,
                system_injection,
                context_level=context_level,  # ä¼ é€’ä¸Šä¸‹æ–‡é™çº§å±‚çº§
                history_override=history_override,
            )

            # è¾“å‡ºå‘é€ç»™æ¨¡å‹çš„å®Œæ•´æ¶ˆæ¯ï¼ˆä»…åœ¨ DEBUG æ¨¡å¼ï¼‰
            self._log_request_messages(messages, api_content, request_id)
            
            # 3. å‘é€è¯·æ±‚
            try:
                assistant_message, tool_calls, api_key = await self._send_api_request(
                    request_body, request_id, retry_count,
                    message, user_id, group_id, image_urls, enable_tools
                )
            except ServerError as e:
                # æœåŠ¡ç«¯é”™è¯¯éœ€è¦é‡è¯•
                retry_reply = await self._handle_server_error_retry(
                    e,
                    message=message,
                    user_id=user_id,
                    group_id=group_id,
                    image_urls=image_urls,
                    enable_tools=enable_tools,
                    retry_count=retry_count,
                    message_id=message_id,
                    system_injection=system_injection,
                    context_level=context_level,
                    history_override=history_override,
                    search_result=search_result,
                )
                if retry_reply is not None:
                    return retry_reply
                raise
            
            # 4. å¤„ç†å·¥å…·è°ƒç”¨ï¼ˆå¦‚æœ‰ï¼‰
            reply, tool_trace = await self._resolve_reply(
                messages=messages,
                assistant_message=assistant_message,
                tool_calls=tool_calls,
                api_key=api_key,
                group_id=group_id,
                request_id=request_id,
                enable_tools=enable_tools,
                tools=request_body.get("tools"),
                search_state=search_state,
            )
            self._log_search_decision(request_id, search_state, phase="post_reply")
            

            # è¾“å‡ºæ¨¡å‹åŸå§‹å›å¤ï¼ˆè°ƒè¯•ç”¨ï¼‰
            self._log_raw_model_reply(reply, request_id)

            # 5. æ¸…ç†æ€è€ƒè¿‡ç¨‹æ ‡è®°/æ ‡ç­¾/ç©ºç™½
            reply = self._process_response(reply, request_id)
            
            # 6. æ£€æŸ¥ç©ºå›å¤ - æ™ºèƒ½é™çº§é‡è¯•æœºåˆ¶
            if not reply:
                empty_meta = assistant_message.get("_empty_reply_meta") if isinstance(assistant_message, dict) else None
                if isinstance(empty_meta, dict):
                    log.warning(
                        f"[req:{request_id}] transport_empty_meta | kind={empty_meta.get('kind', '')} | "
                        f"finish={empty_meta.get('finish_reason', '') or 'unknown'} | "
                        f"local_retries={empty_meta.get('local_retries', 0)} | "
                        f"response_id={empty_meta.get('response_id', '') or '-'}"
                    )
                retry_reply = await self._handle_empty_reply_retry(
                    request_id=request_id,
                    start_time=start_time,
                    message=message,
                    user_id=user_id,
                    group_id=group_id,
                    image_urls=image_urls,
                    enable_tools=enable_tools,
                    retry_count=retry_count,
                    message_id=message_id,
                    system_injection=system_injection,
                    context_level=context_level,
                    history_override=history_override,
                    search_result=search_result,
                )
                if retry_reply is not None:
                    return retry_reply

                # æ‰€æœ‰é™çº§å±‚çº§éƒ½å¤±è´¥äº†
                log.error(f"[req:{request_id}] æ‰€æœ‰ä¸Šä¸‹æ–‡é™çº§å±‚çº§éƒ½å¤±è´¥ï¼Œè¿”å›é”™è¯¯æ¶ˆæ¯")
                return self._get_error_message("empty_reply")
            # æ­¥éª¤ 5: æ›´æ–°ä¸Šä¸‹æ–‡
            # ä½¿ç”¨ä» chat æ‹†åˆ†å‡ºæ¥çš„æ¶ˆæ¯åˆ†ç¦»é€»è¾‘å¾—åˆ°çš„ original_content
            # æ³¨æ„ï¼šå¦‚æœ original_content æ˜¯å¤šæ¨¡æ€åˆ—è¡¨ï¼Œ_update_context éœ€è¦èƒ½å¤„ç†
            await self._update_context(
                user_id,
                group_id,
                original_content,
                reply,
                message_id,
                tool_trace=tool_trace,
            )
            
            # æ—¥å¿—ï¼šç¡®è®¤ä¿å­˜çš„æ˜¯åŸå§‹æ¶ˆæ¯
            if search_result:
                log.debug(
                    f"[req:{request_id}] ä¸Šä¸‹æ–‡å·²æ›´æ–°ï¼ˆå·²åˆ†ç¦»æœç´¢ç»“æœï¼‰| "
                    f"saved_len={len(str(original_content))} | "
                    f"api_len={len(str(api_content))}"
                )
            
            # ç»“æ„åŒ–æ—¥å¿—ï¼šè¯·æ±‚æˆåŠŸ
            self._log_request_success(request_id, start_time, reply, tool_calls)
            
            return reply
            
        except Exception as e:
            return self._handle_error(e, request_id, start_time)


    def _extract_nickname_from_content(self, content: str) -> tuple:
        """
        ä»æ ¼å¼åŒ–çš„æ¶ˆæ¯å†…å®¹ä¸­æå–æ˜µç§°å’Œçº¯å†…å®¹
        
        æ¶ˆæ¯æ ¼å¼: [æ˜µç§°(QQå·)]: æ¶ˆæ¯å†…å®¹ æˆ– [â­Sensei]: æ¶ˆæ¯å†…å®¹
        
        Args:
            content: æ ¼å¼åŒ–çš„æ¶ˆæ¯å†…å®¹
            
        Returns:
            (nickname, pure_content) å…ƒç»„
        """
        import re
        
        if not content:
            return ("User", "")
        
        # åŒ¹é… [æ˜µç§°(QQå·)]: æˆ– [â­Sensei]: æ ¼å¼
        # æ”¯æŒ: [å¼ ä¸‰(123456)]: xxx, [â­Sensei]: xxx, [ç§èŠç”¨æˆ·]: xxx
        pattern = r'^\[([^\]]+)\]:\s*(.*)$'
        match = re.match(pattern, content, re.DOTALL)
        
        if match:
            tag = match.group(1)  # å¦‚ "å¼ ä¸‰(123456)" æˆ– "â­Sensei"
            pure_content = match.group(2)  # çº¯æ¶ˆæ¯å†…å®¹
            
            # è¿›ä¸€æ­¥æå–æ˜µç§°ï¼ˆå»é™¤QQå·éƒ¨åˆ†ï¼‰
            # "å¼ ä¸‰(123456)" -> "å¼ ä¸‰"
            nickname_match = re.match(r'^(.+?)\(\d+\)$', tag)
            if nickname_match:
                nickname = nickname_match.group(1)
            else:
                nickname = tag  # å¦‚ "â­Sensei" æˆ– "ç§èŠç”¨æˆ·"
            
            return (nickname, pure_content)
        
        # æœªåŒ¹é…åˆ°æ ¼å¼ï¼Œè¿”å›åŸå†…å®¹
        return ("User", content)

    def _extract_json_object(self, text: str) -> Optional[str]:
        """ä»æ–‡æœ¬ä¸­æå– JSON å¯¹è±¡ï¼ˆå§”æ´¾åˆ°ç‹¬ç«‹æ¨¡å—ï¼‰ã€‚"""
        return extract_json_object(text)

    async def judge_proactive_intent(self, context_messages: List[Dict], heat_level: int) -> Dict[str, Any]:
        """
        åˆ¤æ–­æ˜¯å¦éœ€è¦ä¸»åŠ¨å‘è¨€
        
        Args:
            context_messages: æœ€è¿‘çš„ç¾¤èŠæ¶ˆæ¯åˆ—è¡¨
            heat_level: å½“å‰çƒ­åº¦å€¼
            
        Returns:
            Dict: {
                "should_reply": bool,
                "reason": str
            }
        """
        import json
        import re
        
        try:
            # 1. åŠ è½½æç¤ºè¯
            prompt_config = load_judge_prompt()
            if not isinstance(prompt_config, dict):
                log.warning(
                    f"[ä¸»åŠ¨å‘è¨€åˆ¤å†³] æç¤ºè¯æ ¹èŠ‚ç‚¹åº”ä¸º dictï¼Œå®é™…ä¸º {type(prompt_config).__name__}ï¼Œå·²ç¦ç”¨æœ¬æ¬¡ä¸»åŠ¨å‘è¨€"
                )
                return {"should_reply": False, "reason": "invalid_prompt_root"}

            judge_config = prompt_config.get("judge_proactive", {})
            if not isinstance(judge_config, dict):
                log.warning(
                    f"[ä¸»åŠ¨å‘è¨€åˆ¤å†³] judge_proactive åº”ä¸º dictï¼Œå®é™…ä¸º {type(judge_config).__name__}ï¼Œå·²ç¦ç”¨æœ¬æ¬¡ä¸»åŠ¨å‘è¨€"
                )
                return {"should_reply": False, "reason": "invalid_judge_prompt"}

            template = judge_config.get("template", "")
            if not isinstance(template, str):
                template = ""
            
            if not template:
                log.warning("ä¸»åŠ¨å‘è¨€åˆ¤å†³æç¤ºè¯åŠ è½½å¤±è´¥")
                return {"should_reply": False, "reason": "No prompt"}
            
            log.info(f"[ä¸»åŠ¨å‘è¨€åˆ¤å†³] æ­£ä½¿ç”¨å¿«é€Ÿæ¨¡å‹: {plugin_config.gemini_fast_model}")
                
            # 2. æ ¼å¼åŒ–ä¸Šä¸‹æ–‡
            # å°† context_messages è½¬æ¢ä¸ºæ˜“è¯»çš„å­—ç¬¦ä¸²æ ¼å¼
            # æ³¨æ„ï¼šå†å²æ¶ˆæ¯æ ¼å¼æ˜¯ [æ˜µç§°(QQå·)]: æ¶ˆæ¯å†…å®¹ï¼Œéœ€è¦è§£æ
            context_str_list = []
            collected_images = []  # æ”¶é›†æœ€è¿‘çš„å›¾ç‰‡
            
            for msg in context_messages:
                content = msg.get("content", "")
                
                # å¤„ç†å¤šæ¨¡æ€å†…å®¹ï¼ˆåˆ—è¡¨ï¼‰
                if isinstance(content, list):
                    # æå–æ–‡æœ¬éƒ¨åˆ†ç”¨äº context_str_list
                    text_part = ""
                    for item in content:
                        if item.get("type") == "text":
                            text_part += item.get("text", "")
                        elif item.get("type") == "image_url":
                            # æ”¶é›†å›¾ç‰‡ï¼ˆä»…æœ€è¿‘çš„ï¼‰
                            collected_images.append(item)
                    
                    # æå–æ˜µç§°é€»è¾‘éœ€è¦é€‚é…çº¯æ–‡æœ¬éƒ¨åˆ†
                    sender, pure_content = self._extract_nickname_from_content(text_part)
                else:
                    # æ™®é€šæ–‡æœ¬
                    sender, content_str = self._extract_nickname_from_content(content)
                    pure_content = content_str
                
                # ä¼˜å…ˆä½¿ç”¨å·²æœ‰çš„ nickname å­—æ®µ
                if "nickname" in msg:
                    sender = msg["nickname"]
                
                # [å…³é”®ä¼˜åŒ–] å¦‚æœæ˜¯ Bot è‡ªå·±çš„æ¶ˆæ¯ï¼Œå¼ºåˆ¶æ ‡è®°ä¸º Mika
                if msg.get("role") == "assistant":
                    sender = "Mika"
                
                context_str_list.append(f"{sender}: {pure_content}")
            
            context_text = "\n".join(context_str_list)
            
            # 3. æ„å»º Prompt
            prompt_text = template.format(
                heat_level=heat_level,
                context_messages=context_text
            )
            
            # æ„å»ºå¤šæ¨¡æ€ Content
            final_content = [{"type": "text", "text": prompt_text}]
            
            # é™„åŠ æœ€è¿‘çš„å›¾ç‰‡ï¼ˆæœ€å¤š2å¼ ï¼Œé˜²æ­¢ Token çˆ†ç‚¸ï¼‰
            if collected_images:
                # å–æœ€å N å¼ ï¼ˆé»˜è®¤ 2ï¼‰ï¼Œé˜²æ­¢ Token çˆ†ç‚¸
                recent_images = collected_images[-plugin_config.gemini_proactive_judge_max_images :]
                final_content.extend(recent_images)
                log.debug(f"[ä¸»åŠ¨å‘è¨€åˆ¤å†³] åŒ…å«å›¾ç‰‡è¾“å…¥ | count={len(recent_images)}")
            
            # 4. è°ƒç”¨ API
            current_api_key = self._get_api_key()
            messages = [{"role": "user", "content": final_content}]
            
            client = await self._get_client()
            raw_content = "" # åˆå§‹åŒ–é¿å…ä½œç”¨åŸŸé—®é¢˜
            
            # [é‡è¯•æœºåˆ¶] å¯¹ç½‘ç»œé”™è¯¯è¿›è¡Œé‡è¯•
            import asyncio
            max_retries = plugin_config.gemini_proactive_judge_max_retries
            retry_delay = plugin_config.gemini_proactive_judge_retry_delay_seconds  # åˆå§‹é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
            last_error = None
            response = None
            
            for attempt in range(max_retries + 1):
                try:
                    response = await client.post(
                        f"{self.base_url}/chat/completions",
                        headers={
                            "Authorization": f"Bearer {current_api_key}",
                            "Content-Type": "application/json"
                        },
                        json={
                            "model": plugin_config.gemini_fast_model, # ä½¿ç”¨å¿«é€Ÿæ¨¡å‹
                            "messages": messages,
                            "temperature": plugin_config.gemini_proactive_temperature, # ä½¿ç”¨é…ç½®çš„åˆ¤å†³æ¸©åº¦
                            "response_format": {"type": "json_object"}, # å¼ºåˆ¶ JSON
                            "stream": False, # æ˜¾å¼ç¦ç”¨æµå¼ä¼ è¾“
                            "safetySettings": [ # [å®‰å…¨è®¾ç½®] åŒæ ·åº”ç”¨ BLOCK_NONE
                                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"}
                            ]
                        },
                        timeout=plugin_config.gemini_proactive_judge_timeout_seconds  # é»˜è®¤ 20s
                    )
                    break  # æˆåŠŸåˆ™è·³å‡ºé‡è¯•å¾ªç¯
                except (httpx.ConnectError, httpx.NetworkError, httpx.TimeoutException) as e:
                    last_error = e
                    if attempt < max_retries:
                        log.warning(
                            f"[ä¸»åŠ¨å‘è¨€åˆ¤å†³] ç½‘ç»œé”™è¯¯ï¼Œç¬¬ {attempt + 1} æ¬¡é‡è¯• | "
                            f"error={type(e).__name__}: {str(e)[:PROACTIVE_JUDGE_ERROR_PREVIEW_CHARS]}"
                        )
                        await asyncio.sleep(retry_delay)
                        retry_delay *= 2  # æŒ‡æ•°é€€é¿
                    else:
                        log.error(
                            f"[ä¸»åŠ¨å‘è¨€åˆ¤å†³] ç½‘ç»œé”™è¯¯ï¼Œé‡è¯• {max_retries} æ¬¡åä»å¤±è´¥ | "
                            f"error={type(e).__name__}: {str(e)[:PROACTIVE_JUDGE_ERROR_PREVIEW_CHARS]}"
                        )
                        return {"should_reply": False, "reason": f"network_error: {type(e).__name__}"}
            
            if response is None:
                # ä¸åº”è¯¥åˆ°è¾¾è¿™é‡Œï¼Œä½†ä½œä¸ºå®‰å…¨ä¿æŠ¤
                return {"should_reply": False, "reason": "no_response"}
            
            response.raise_for_status()
            data = response.json()
            
            # å®‰å…¨è·å– content
            raw_content = data.get("choices", [{}])[0].get("message", {}).get("content", "")
            
            # ç©ºå€¼æ£€æŸ¥
            if not raw_content or not raw_content.strip():
                # Extract finish reason for debugging
                finish_reason = data.get("choices", [{}])[0].get("finish_reason", "UNKNOWN")
                log.warning(f"[ä¸»åŠ¨å‘è¨€åˆ¤å†³] API è¿”å›ç©ºå†…å®¹ | finish_reason={finish_reason} | Full Data: {data}")
                return {"should_reply": False}
            
            # æ¸…ç† Markdown ä»£ç å—åŒ…è£¹
            clean_content = re.sub(r'^```(?:json)?\s*', '', raw_content.strip())
            clean_content = re.sub(r'```\s*$', '', clean_content)
            clean_content = clean_content.strip()
            
            if not clean_content:
                log.warning(
                    f"[ä¸»åŠ¨å‘è¨€åˆ¤å†³] å†…å®¹æ¸…ç†åä¸ºç©º | åŸå§‹: {raw_content[:PROACTIVE_JUDGE_RAW_CONTENT_SHORT_PREVIEW_CHARS]}"
                )
                return {"should_reply": False}

            # å°è¯•æå– JSON å¯¹è±¡ï¼ˆå¥å£®æ¨¡å¼ï¼‰
            extracted_json = self._extract_json_object(clean_content)
            if extracted_json:
                clean_content = extracted_json
            
            # ä½¿ç”¨æ›´å®½å®¹çš„ JSON parser (å¦‚ json5) å¦‚æœæœ‰ï¼Œè¿™é‡Œæš‚æ—¶ç”¨æ ‡å‡†åº“
            # æ³¨æ„ï¼šæ ‡å‡† json ä¸æ”¯æŒ trailing commaï¼Œå¦‚æœ LLM è¾“å‡ºä¸è§„èŒƒå¯èƒ½ä¼šæŒ‚
            # ä½† _extract_json_object å·²ç»å¾ˆå¤§ç¨‹åº¦è§£å†³äº†åµŒå¥—æ‹¬å·é—®é¢˜
            result = json.loads(clean_content)
            
            log.info(f"[ä¸»åŠ¨å‘è¨€åˆ¤å†³] ç»“æœ: {result.get('should_reply')}")
            return result
            
        except json.JSONDecodeError as e:
            # è¯¦ç»†è®°å½• JSON è§£æå¤±è´¥çš„æƒ…å†µ
            # å¦‚æœ raw_content ä¸ºç©ºï¼Œè¯´æ˜å¯èƒ½æ˜¯ response.json() é˜¶æ®µè¿™å°±æŒ‚äº†ï¼Œæ‰“å° response.text
            if not raw_content and 'response' in locals():
                try:
                    server_response = response.text[:PROACTIVE_JUDGE_SERVER_RESPONSE_PREVIEW_CHARS]
                    log.error(f"[ä¸»åŠ¨å‘è¨€åˆ¤å†³] API å“åº”è§£æå¤±è´¥ (HTTP {response.status_code}): {e} | Body: {server_response}")
                    return {"should_reply": False}
                except Exception:
                    pass

            raw_preview = (
                raw_content[:PROACTIVE_JUDGE_RAW_CONTENT_ERROR_PREVIEW_CHARS]
                if raw_content
                else '(DEBUG_EMPTY_CONTENT)'
            )
            log.error(f"[ä¸»åŠ¨å‘è¨€åˆ¤å†³] JSON è§£æå¤±è´¥: {e} | åŸå§‹å†…å®¹: {raw_preview} | Hex: {raw_content.encode('utf-8').hex() if raw_content else 'None'}")
            return {"should_reply": False}
        except Exception as e:
            import traceback
            log.error(f"[ä¸»åŠ¨å‘è¨€åˆ¤å†³] å¤±è´¥: {repr(e)}")
            log.error(traceback.format_exc())
            return {"should_reply": False}
